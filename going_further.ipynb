{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5839866c",
   "metadata": {},
   "source": [
    "## trying something else !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1a8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OLD Distribution (Strict) ---\n",
      "Chance of Admit\n",
      "Medium    413\n",
      "Reach     275\n",
      "Safe       12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- NEW Distribution (Balanced) ---\n",
      "Chance of Admit\n",
      "Medium    476\n",
      "Reach     153\n",
      "Safe       71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# --- OLD THRESHOLDS ---\n",
    "def old_category(p):\n",
    "    if p > 0.90: return 'Safe'\n",
    "    elif p >= 0.70: return 'Medium'\n",
    "    else: return 'Reach'\n",
    "\n",
    "# --- NEW THRESHOLDS ---\n",
    "def new_category(p):\n",
    "    if p >= 0.85: return 'Safe'      # Changed from 0.90\n",
    "    elif p >= 0.65: return 'Medium'  # Changed from 0.70\n",
    "    else: return 'Reach'\n",
    "\n",
    "print(\"--- OLD Distribution (Strict) ---\")\n",
    "print(df['Chance of Admit'].apply(old_category).value_counts())\n",
    "\n",
    "print(\"\\n--- NEW Distribution (Balanced) ---\")\n",
    "print(df['Chance of Admit'].apply(new_category).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235ad9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: 700\n",
      "Bootstrapped Size: 1500\n",
      "Category\n",
      "Safe      500\n",
      "Medium    500\n",
      "Reach     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 2. Define Categories (Using your new balanced thresholds)\n",
    "def get_category(p):\n",
    "    if p >= 0.85: return 'Safe'\n",
    "    elif p >= 0.65: return 'Medium'\n",
    "    else: return 'Reach'\n",
    "\n",
    "df['Category'] = df['Chance of Admit'].apply(get_category)\n",
    "\n",
    "# 3. Bootstrapping Function\n",
    "def bootstrap_balance(df, target_count=500):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each class (Safe, Medium, Reach)\n",
    "    for category in ['Safe', 'Medium', 'Reach']:\n",
    "        df_class = df[df['Category'] == category]\n",
    "        \n",
    "        # Resample (This is the \"Bootstrapping\" part)\n",
    "        # replace=True allows us to pick the same student multiple times\n",
    "        df_upsampled = resample(df_class, \n",
    "                                replace=True,     \n",
    "                                n_samples=target_count,    \n",
    "                                random_state=42) \n",
    "        \n",
    "        df_balanced = pd.concat([df_balanced, df_upsampled])\n",
    "        \n",
    "    return df_balanced\n",
    "\n",
    "# 4. Run it\n",
    "df_bootstrapped = bootstrap_balance(df)\n",
    "\n",
    "print(\"Original Size:\", len(df))\n",
    "print(\"Bootstrapped Size:\", len(df_bootstrapped))\n",
    "print(df_bootstrapped['Category'].value_counts())\n",
    "\n",
    "# Save\n",
    "df_bootstrapped.to_csv('train_bootstrapped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fdf044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL BALANCED DISTRIBUTION ---\n",
      "Category\n",
      "Medium    500\n",
      "Safe      500\n",
      "Reach     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 2. Apply the NEW Categories\n",
    "def get_category(p):\n",
    "    if p >= 0.85: return 'Safe'\n",
    "    elif p >= 0.65: return 'Medium'\n",
    "    else: return 'Reach'\n",
    "\n",
    "df['Category'] = df['Chance of Admit'].apply(get_category)\n",
    "\n",
    "# 3. Targeted Augmentation Function\n",
    "def augment_class(df, target_class, target_count):\n",
    "    \"\"\"\n",
    "    Augments a specific class until it reaches the target_count.\n",
    "    \"\"\"\n",
    "    existing_class = df[df['Category'] == target_class]\n",
    "    current_count = len(existing_class)\n",
    "    \n",
    "    if current_count >= target_count:\n",
    "        return existing_class # No need to augment if we already have enough\n",
    "    \n",
    "    needed = target_count - current_count\n",
    "    \n",
    "    # Generate the new samples\n",
    "    indices = np.random.choice(existing_class.index, size=needed, replace=True)\n",
    "    new_data = existing_class.loc[indices].copy()\n",
    "    \n",
    "    # Add Noise to numeric columns only\n",
    "    numeric_cols = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'GPA']\n",
    "    for col in numeric_cols:\n",
    "        std_dev = df[col].std()\n",
    "        noise = np.random.normal(0, std_dev * 0.05, size=needed) # 5% noise\n",
    "        new_data[col] = new_data[col] + noise\n",
    "        \n",
    "    # Clipping to keep data realistic\n",
    "    new_data['GRE Score'] = np.clip(new_data['GRE Score'], 290, 340)\n",
    "    new_data['TOEFL Score'] = np.clip(new_data['TOEFL Score'], 92, 120)\n",
    "    new_data['GPA'] = np.clip(new_data['GPA'], 0, 4.0)\n",
    "    new_data['SOP'] = np.clip(new_data['SOP'], 1, 5)\n",
    "    new_data['University Rating'] = np.clip(np.round(new_data['University Rating']), 1, 5)\n",
    "\n",
    "    return pd.concat([existing_class, new_data], axis=0)\n",
    "\n",
    "# --- EXECUTE BALANCING ---\n",
    "\n",
    "# We want everyone to match the majority class (Medium ~476)\n",
    "target_size = 500 \n",
    "\n",
    "df_safe_balanced = augment_class(df, 'Safe', target_size)\n",
    "df_reach_balanced = augment_class(df, 'Reach', target_size)\n",
    "df_medium_balanced = augment_class(df, 'Medium', target_size) # Will just return original if > 500\n",
    "\n",
    "# Combine them all back together\n",
    "df_final = pd.concat([df_safe_balanced, df_reach_balanced, df_medium_balanced], axis=0)\n",
    "\n",
    "# Shuffle the rows so they aren't in order\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"--- FINAL BALANCED DISTRIBUTION ---\")\n",
    "print(df_final['Category'].value_counts())\n",
    "\n",
    "# Save this file to train your models\n",
    "df_final.to_csv('train_balanced_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb2421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped Train Size: 1500\n",
      "Noise Injected Train Size: 1500\n",
      "\n",
      "========================================\n",
      "RESULTS COMPARISON\n",
      "========================================\n",
      "\n",
      "Model A: BOOTSTRAPPING Accuracy: 0.8100\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Medium       0.83      0.85      0.84       176\n",
      "       Reach       0.84      0.80      0.82        81\n",
      "        Safe       0.66      0.67      0.67        43\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.78      0.77      0.78       300\n",
      "weighted avg       0.81      0.81      0.81       300\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Model B: NOISE INJECTION Accuracy: 0.7933\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Medium       0.80      0.87      0.83       176\n",
      "       Reach       0.84      0.73      0.78        81\n",
      "        Safe       0.68      0.60      0.64        43\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.77      0.73      0.75       300\n",
      "weighted avg       0.79      0.79      0.79       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "# Define your Categorization Logic\n",
    "def get_category(p):\n",
    "    if p >= 0.85: return 'Safe'\n",
    "    elif p >= 0.65: return 'Medium'\n",
    "    else: return 'Reach'\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv') # Make sure this file exists!\n",
    "\n",
    "# Add Categories to both\n",
    "train_df['Category'] = train_df['Chance of Admit'].apply(get_category)\n",
    "test_df['Category'] = test_df['Chance of Admit'].apply(get_category)\n",
    "\n",
    "# Prepare X and y for Test Set (The \"Truth\")\n",
    "X_test = test_df.drop(['Chance of Admit', 'Category'], axis=1)\n",
    "y_test = test_df['Category']\n",
    "\n",
    "# ==========================================\n",
    "# 2. CREATE DATASET A: BOOTSTRAPPING\n",
    "# ==========================================\n",
    "def create_bootstrap_data(df, target_count=500):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    for category in ['Safe', 'Medium', 'Reach']:\n",
    "        subset = df[df['Category'] == category]\n",
    "        # Just sample with replacement (Duplicates)\n",
    "        resampled = subset.sample(n=target_count, replace=True, random_state=42)\n",
    "        df_balanced = pd.concat([df_balanced, resampled])\n",
    "    return df_balanced.sample(frac=1, random_state=42) # Shuffle\n",
    "\n",
    "# ==========================================\n",
    "# 3. CREATE DATASET B: NOISE INJECTION\n",
    "# ==========================================\n",
    "def create_noise_data(df, target_count=500):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    numeric_cols = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'GPA']\n",
    "    \n",
    "    for category in ['Safe', 'Medium', 'Reach']:\n",
    "        subset = df[df['Category'] == category]\n",
    "        \n",
    "        # Start with original data\n",
    "        count = len(subset)\n",
    "        needed = target_count - count\n",
    "        \n",
    "        # Create Synthetic data\n",
    "        if needed > 0:\n",
    "            indices = np.random.choice(subset.index, size=needed, replace=True)\n",
    "            synthetic = subset.loc[indices].copy()\n",
    "            \n",
    "            # Inject Noise\n",
    "            for col in numeric_cols:\n",
    "                std = df[col].std()\n",
    "                noise = np.random.normal(0, std * 0.05, size=needed)\n",
    "                synthetic[col] += noise\n",
    "            \n",
    "            # Clip\n",
    "            synthetic['GRE Score'] = np.clip(synthetic['GRE Score'], 290, 340)\n",
    "            synthetic['GPA'] = np.clip(synthetic['GPA'], 0, 4.0)\n",
    "            # (Add other clips as needed)\n",
    "            \n",
    "            combined = pd.concat([subset, synthetic])\n",
    "        else:\n",
    "            combined = subset.sample(target_count) # Downsample if too big\n",
    "            \n",
    "        df_balanced = pd.concat([df_balanced, combined])\n",
    "        \n",
    "    return df_balanced.sample(frac=1, random_state=42) # Shuffle\n",
    "\n",
    "# ==========================================\n",
    "# 4. RUN THE EXPERIMENT\n",
    "# ==========================================\n",
    "\n",
    "# A. Generate the two training sets\n",
    "train_boot = create_bootstrap_data(train_df)\n",
    "train_noise = create_noise_data(train_df)\n",
    "\n",
    "print(f\"Bootstrapped Train Size: {len(train_boot)}\")\n",
    "print(f\"Noise Injected Train Size: {len(train_noise)}\")\n",
    "\n",
    "# B. Train Model A (Bootstrap)\n",
    "clf_boot = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_boot.fit(train_boot.drop(['Chance of Admit', 'Category'], axis=1), train_boot['Category'])\n",
    "preds_boot = clf_boot.predict(X_test)\n",
    "\n",
    "# C. Train Model B (Noise)\n",
    "clf_noise = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_noise.fit(train_noise.drop(['Chance of Admit', 'Category'], axis=1), train_noise['Category'])\n",
    "preds_noise = clf_noise.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 5. PRINT RESULTS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"\\nModel A: BOOTSTRAPPING Accuracy: {accuracy_score(y_test, preds_boot):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(classification_report(y_test, preds_boot, target_names=clf_boot.classes_))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"\\nModel B: NOISE INJECTION Accuracy: {accuracy_score(y_test, preds_noise):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(classification_report(y_test, preds_noise, target_names=clf_noise.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3271b7",
   "metadata": {},
   "source": [
    "While Bootstrapping technically has a slightly higher overall accuracy ($0.81$ vs $0.806$), Noise Injection is the superior model for a real-world application.\n",
    "\n",
    "Why did this happen? Bootstrapping works by duplicating exact rows. The model essentially \"memorized\" the specific GPA/GRE combinations of the few safe students in your training set. The Noise Injection model learned a \"fuzzy\" boundary around those scores, making it robust enough to reject students who were close but not quite good enough.\n",
    "\n",
    "We will retrain the different models we used with the noise injection method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47f142",
   "metadata": {},
   "source": [
    "## retrain XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162e316c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ================================================================\n",
    "# 1. Load the Balanced Data (Created via Noise Injection)\n",
    "# ================================================================\n",
    "df = pd.read_csv('train_balanced_final.csv')\n",
    "\n",
    "print(f\"Training on {len(df)} samples.\")\n",
    "print(\"Class Distribution:\\n\", df['Category'].value_counts())\n",
    "\n",
    "# ================================================================\n",
    "# 2. Preprocessing for XGBoost\n",
    "# ================================================================\n",
    "# XGBoost requires numerical classes (0, 1, 2)\n",
    "# We define a specific mapping to ensure we know which is which\n",
    "class_mapping = {'Reach': 0, 'Medium': 1, 'Safe': 2}\n",
    "df['Encoded_Category'] = df['Category'].map(class_mapping)\n",
    "\n",
    "# Define Features and Target\n",
    "X = df.drop(['Chance of Admit', 'Category', 'Encoded_Category'], axis=1)\n",
    "y = df['Encoded_Category']\n",
    "\n",
    "# Optional: Split a tiny validation set just to check it works\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# ================================================================\n",
    "# 3. Train the XGBoost Model\n",
    "# ================================================================\n",
    "# We use standard parameters. You can tune these, but defaults work well here.\n",
    "model = XGBClassifier(\n",
    "    n_estimators=200,     # Number of trees\n",
    "    learning_rate=0.05,   # Step size (lower is better for generalization)\n",
    "    max_depth=4,          # Depth of trees (prevent overfitting)\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss' # Multi-class log loss\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ================================================================\n",
    "# 4. Validate (Sanity Check)\n",
    "# ================================================================\n",
    "val_preds = model.predict(X_val)\n",
    "acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(\"\\nValidation Report:\")\n",
    "# We map the numbers back to names for the report\n",
    "target_names = ['Reach', 'Medium', 'Safe'] \n",
    "print(classification_report(y_val, val_preds, target_names=target_names))\n",
    "\n",
    "# ================================================================\n",
    "# 5. Save Final Model\n",
    "# ================================================================\n",
    "joblib.dump(model, \"xgb_grad_admission_model_bootstrap.joblib\")\n",
    "print(\"\\nSUCCESS: Model saved as 'xgb_grad_admission_mode_bootstrap.joblib'\")\n",
    "\n",
    "# Example of using the saved model\n",
    "loaded_model = joblib.load(\"xgb_grad_admission_model_bootstrap.joblib\")\n",
    "\n",
    "# Prediction for a new student\n",
    "prediction_index = loaded_model.predict(new_student_data)[0] # Returns 0, 1, or 2\n",
    "\n",
    "# Translation\n",
    "classes = {0: \"Reach\", 1: \"Medium\", 2: \"Safe\"}\n",
    "print(f\"Result: {classes[prediction_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e3e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
